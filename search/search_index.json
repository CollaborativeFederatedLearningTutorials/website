{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Federated Learning for Healthcare","text":"<p>Welcome to the Federated Learning tutorial that will be run in conjunction with multiple conferences!</p> <p>Federated Learning (FL) is increasingly important in privacy sensitive domains, such as healthcare, where sharing of private/patient data is a barrier to building models that generalize well in the real world and minimize bias.</p> <p>In this tutorial, we will be presenting the COmprehensive Federated Ecosystem (COFE), which comprises of the following components:</p> <ol> <li>The Generally Nuanced Deep Learning Framework (GaNDLF) - gandlf.org</li> <li>MedPerf - www.medperf.org</li> <li>OpenFL - github.com/securefederatedai/openfl</li> <li>Hugging Face Hub - huggingface.co</li> </ol> <p>In 2021, COFE was used to conduct the largest to-date real world federation, with a network of 71 healthcare institutions around the world, the Federated Tumor Segmentation (FeTS) Initiative [ref]. Furthermore, leveraging the collaborators of this real-world FL initiative, the first ever FL challenge was conducted, which focused on the tumor segmentation task, called The FeTS 2021 challenge [ref], which was conducted again in 2022 [ref]. Taking into consideration the value and the interest of the community in this new paradigm for data private multi-institutional collaborations and building upon our experience, we organize this tutorial on FL for healthcare.</p>"},{"location":"#upcoming-events","title":"Upcoming Events","text":"<ul> <li>MICCAI 2025 - 23-27 September (South Korea)</li> </ul>"},{"location":"#past-events","title":"Past Events","text":"<ul> <li>MICCAI 2024 - 6-10 October (Morocco)</li> <li>ISBI 2024 - 27 May (Greece)</li> <li>Bridge Event at AAAI 2024 - 19-20 Feb (Canada)</li> <li>Hands on Deep Learning Lab at RSNA Annual Meet 2023 - Nov (Chicago)</li> <li>Tutorial in MICCAI 2023</li> <li>Tutorial in MICCAI 2022</li> </ul>"},{"location":"#organizing-committee","title":"Organizing Committee","text":"<ul> <li>Sarthak Pati, MLCommons.</li> <li>Patrick Foley.</li> <li>Hasan Kassem, MLCommons.</li> <li>Alex Karargyris, MLCommons.</li> <li>Spyridon Bakas, Indiana University &amp; MLCommons.</li> </ul>"},{"location":"#tutorial-description","title":"Tutorial Description","text":"<p>The aim of this tutorial is to facilitate education on how to perform Federated Learning on both simulated and real-world studies. Tutorial structure focuses on specific clearly indicated parts for beginners and for more advanced attendees. Data scientists of different medical imaging communities (e.g., radiology, pathology) are considered during this tutorial on the opportunities and challenges in developing and using FL for training Al models across institutions using privacy preserving techniques. We plan on covering a spectrum of techniques, from software-based approaches that can be considered a method or a metric (e.g., differential privacy), to hardware-based trusted execution computing environments (TEEs).</p> <p>The motivation for the tutorial is driven by the need to train and validate deep learning models across data silos, to create models that gain knowledge from diverse patient populations and hence generalize well, mitigate bias, and pave the way towards addressing health disparities.</p>"},{"location":"instructions/","title":"Federated Evaluation Tutorial","text":"<p>Please follow the tutorial on this page</p>"},{"location":"previous/","title":"Previous Tutorials","text":""},{"location":"previous/#2024","title":"2024","text":"<ul> <li>Association for the Advancement of Artificial Intelligence (AAAI)</li> <li>International Symposium of Biomedical Imaging (ISBI)</li> <li>Medical Image Computing and Computer Assisted Intervention (MICCAI)</li> </ul>"},{"location":"previous/#2023","title":"2023","text":"<ul> <li>Radiological Society of North America (RSNA)</li> </ul>"},{"location":"schedule/","title":"Schedule for MICCAI 2025","text":"Time Session Speaker Comments 1330 - 1350 Introduction to MLCommons Alex Karargyris 15 min talk + Q&amp;A 1350 - 1410 Federated Learning in Medical Imaging Spyridon Bakas 15 min talk + Q&amp;A 1410 - 1440 Practical Use of FL with NVFlare for Large Foundation Models Holger Roth 25 min talk + Q&amp;A 1440 - 1510 FL Training using FedBioMed Sergen Cansiz 25 min talk + Q&amp;A 1510 - 1530 Clinical Translation of Federated Learning with Regulatory Overshight Valerio Pascucci 15 min talk + Q&amp;A 1530 - 1600 COFFEE BREAK N.A. Light refreshments available near session rooms 1600 - 1700 Hands-on: NV Flare Holger Roth Demo using slides &amp; Codespaces 1700 - 1800 Hands-on: Federated Benchmarking with MedPerf Alex Karargyris Demo using slides &amp; Codespaces"},{"location":"training_tutorial/","title":"Hands-on Tutorial for Federated Training with Nvidia Flare","text":""},{"location":"training_tutorial/#run-in-codespaces","title":"Run in codespaces","text":"<p>After opening the link, proceed to creating the codespace without changing any option. It will take around 7 minutes to get the codespace up and running.</p>"},{"location":"training_tutorial/#overview","title":"Overview","text":"<p>In this guide, you will learn how you can run a training experiment with MedPerf and Nvidia Flare. You will play two roles in this tutorial: the role of an experiment admin, and the role of a data owner.</p> <p>The main tasks of this guide are:</p> <ol> <li>Experiment Admin: Training Experiment Definition</li> <li>Experiment Admin: Aggregator Setup</li> <li>Experiment Admin: training configuration</li> <li>Experiment Admin: Starting the Aggregator</li> <li>Data Owner: Registering and Preparing the Data</li> <li>Data Owner: Requesting participation</li> <li>Data Owner: Starting the training node</li> <li>Data Owner: Setting up the second data owner</li> <li>Experiment Admin: Starting the training</li> <li>Experiment Admin: Remote status check</li> <li>Experiment Admin: Stopping the training</li> </ol>"},{"location":"training_tutorial/#1-experiment-admin-training-experiment-definition","title":"1. Experiment Admin: Training Experiment Definition","text":"<p>First, login as the experiment admin to proceed:</p> <pre><code>medperf auth login -e testmo@example.com\n</code></pre> <p>You will first need to define the logic for data preparation, and the logic for training. Then, register the training experiment object.</p>"},{"location":"training_tutorial/#register-the-data-preparation-container","title":"Register the data preparation Container","text":"<p>As an experiment admin, you should prepare the data preparation pipeline logic that will transform the raw clinical data into AI-ready data. For this tutorial, it will be a container that transforms chest x-ray images and disease labels into numpy arrays. The container is already prepared and hosted. Run the following command to register the container in the MedPerf server:</p> <pre><code>medperf container submit --name prep \\\n--container-config-file \"https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/data_preparator/container_config.yaml\" \\\n--parameters-file \"https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/data_preparator/workspace/parameters.yaml\"\n</code></pre> <p>Here is a description of what has been registered:</p> <ul> <li>The first URL (<code>-m</code> option) points to the container configuration file for the data preparation container. The configuration file contains information like the docker image identifier, and what will be mounted during runtime.</li> <li>The second URL (<code>-p</code> option) points to a parameters file. The data preparation container is built so that it expects certain parameters read from this file.</li> </ul>"},{"location":"training_tutorial/#define-the-training-containers","title":"Define the training Containers","text":"<p>As an experiment admin, you should prepare the necessary training code. Two containers are required: a container that will be used by the Aggregator server and the participating data owners, and a container used by you as an admin to manage the federation (i.e., check the status, start the training, stop the training, etc...).</p> <p>For this tutorial, the containers are already prepared and hosted. Run the following commands below to register the containers in the MedPerf server. Again, the URL (<code>-m</code> option) points to the container configuration file. The configuration file contains information like the docker image identifier, and what will be mounted during runtime.</p>"},{"location":"training_tutorial/#register-the-training-nodes-container","title":"Register the Training nodes Container","text":"<pre><code>medperf container submit --name trainer \\\n--container-config-file \"https://raw.githubusercontent.com/hasan7n/medperf/974879e8eee24ef673a3b0934c80d457ca1ed25c/examples/nvfl/fl/node/container_config.yaml\"\n</code></pre>"},{"location":"training_tutorial/#register-the-training-admin-container","title":"Register the Training admin Container","text":"<pre><code>medperf container submit --name trainadmin \\\n--container-config-file \"https://raw.githubusercontent.com/hasan7n/medperf/974879e8eee24ef673a3b0934c80d457ca1ed25c/examples/nvfl/fl/admin/container_config.yaml\"\n</code></pre>"},{"location":"training_tutorial/#register-the-training-experiment","title":"Register the Training Experiment","text":"<p>Note the IDs of the data preparation container, the training nodes container, and the training admin container. The IDs will be <code>2</code>, <code>3</code>, and <code>4</code> for this tutorial. You can check by running <code>medperf container ls --mine</code>.</p> <p>Now submit the training experiment object, giving it a name, a description, and the containers mentioned above:</p> <pre><code>medperf training submit --name trainexp --description trainexp \\\n--prep-container 2 \\\n--fl-container 3 \\\n--fl-admin-container 4\n</code></pre>"},{"location":"training_tutorial/#associate-a-certificate-authority","title":"Associate a Certificate Authority","text":"<p>Although currently the setup will not use a certificate authority, this step will be needed in an upcoming MedPerf feature where the training startup kits will be securely transported through the MedPerf server. The current NV Flare - MedPerf integration code still expects a certificate authority to be associated with the training experiment.</p> <p>Run the following command to associate a certificate authority:</p> <pre><code>medperf ca associate -t 1 -c 1 -y\n</code></pre>"},{"location":"training_tutorial/#2-experiment-admin-aggregator-setup","title":"2. Experiment Admin: Aggregator Setup","text":"<p>The operator of the aggregator server can be another user, not necessarily the admin. This tutorial will assume it's the same user for simplicity.</p> <p>In this section, you will be registering the aggregator information and linking it to the training experiment.</p>"},{"location":"training_tutorial/#register-the-aggregator","title":"Register the Aggregator","text":"<p>When registering the aggregator information, you should register the address of the aggregator and the ports to be open when the aggregator server will start. We will choose two ports: <code>8102</code> and <code>8103</code>: one for training and one for admin interactions.</p> <p>Note that the hostname should be reachable by the data owners. In a real experiment, it should be the public IP address or a registered domain name of the machine where the aggregator server will be hosted. For the tutorial, we will use an internal IP address since we will run everything in one machine. Run the following to get the hostname:</p> <pre><code>hostname -I | cut -d \" \" -f 1\n</code></pre> <p>Also, you will need to specify which container to use for running the aggregator later. For our case it's going to be the same node container submitted for the training experiment, which is the container of ID <code>3</code>.</p> <p>Now register the aggregator:</p> <pre><code>medperf aggregator submit --name myaggreg --aggregation-container 3 --address &lt;hostname_found&gt; --port 8102 --port 8103\n</code></pre>"},{"location":"training_tutorial/#associate-the-aggregator-with-the-experiment","title":"Associate the aggregator with the experiment","text":"<p>The command below will link the aggregator with the training experiment you already submitted. By running <code>medperf training ls --mine</code> you will see your training experiment ID. By running <code>medperf aggregator ls --mine</code> you will see your aggregator ID. For this tutorial, the IDs will be both <code>1</code>.</p> <pre><code>medperf aggregator associate --aggregator_id 1 --training_exp_id 1\n</code></pre>"},{"location":"training_tutorial/#3-experiment-admin-training-configuration","title":"3. Experiment Admin: training configuration","text":"<p>In this section you will submit the required configuration for training. For our example with NV Flare, the configuration mainly consists of:</p> <ul> <li>The NV Flare's training job configuration (<code>meta.json</code>, <code>config_fed_client.json</code>, and <code>config_fed_server.json</code>). It will be submitted as a single plan file to the MedPerf server, so that data owners can review it before starting training.</li> <li>The startup kits for the aggregator and for the data owners will be created and submitted to the MedPerf server so that the data owners and the aggregator will later automatically pull them and use them when starting their nodes.</li> </ul> <p>Note</p> <p>In an upcoming MedPerf release, the startup kits will be encrypted end-to-end so that they are securely transported from the admin to the data owners through MedPerf.</p> <p>You will need to provide a folder for the following command that includes the necesary configuration files. You can check the folder <code>medperf_tutorial/training_config</code> to see how it's expected to be.</p> <p>Now run the command below:</p> <pre><code>medperf training set_plan --training_exp_id 1 --config-path \"medperf_tutorial/training_config\"\n</code></pre>"},{"location":"training_tutorial/#start-a-training-event","title":"Start a Training Event","text":"<p>After setting the configuration, you should mark your training experiment as ready to accept data owners. The command below will do the job. You will need to provide the expected list of data owners in a yaml file. This file is already prepared for this tutorial and can be found at <code>medperf_tutorial/cols_list.yaml</code>.</p> <p>Run the following command to start the training event:</p> <pre><code>medperf training start_event --name event1 --training_exp_id 1 --participants_list_file medperf_tutorial/cols_list.yaml\n</code></pre>"},{"location":"training_tutorial/#4-experiment-admin-starting-the-aggregator","title":"4. Experiment Admin: Starting the Aggregator","text":"<p>Now let's start the aggregator server:</p> <pre><code>medperf aggregator start --training_exp_id 1 --publish_on &lt;found hostname&gt;\n</code></pre> <p>The <code>--publish_on</code> argument specifies on which network interface to expose the aggregator. For the tutorial, using the hostname found will expose the aggregator to the internal network. In a real experiment where data owners are on different external machines, use <code>0.0.0.0</code>.</p> <p>Please keep this terminal open and move to another terminal to start playing the data owner role.</p>"},{"location":"training_tutorial/#5-data-owner-registering-and-preparing-the-data","title":"5. Data Owner: Registering and Preparing the Data","text":"<p>Now let's play the role of the data owners. There will be two data owners: <code>testdo@example.com</code> and <code>testdo2@example.com</code>. You will learn how to play the role of the data owner using the first one. We provide a shortcut script to automatically run what's needed for the second data owner to avoid repetition.</p> <p>Login as the first data owner:</p> <pre><code>medperf auth logout\nmedperf auth login -e testdo@example.com\n</code></pre>"},{"location":"training_tutorial/#register-your-dataset","title":"Register your dataset","text":"<p>We provide toy dataset of chest x-ray images and labels in the <code>medperf_tutorial</code> folder. Run the following to register the information of this dataset.</p> <pre><code>medperf dataset submit --data_prep 2 \\\n--data_path medperf_tutorial/col1/data \\\n--labels_path medperf_tutorial/col1/labels \\\n--name col1data \\\n--description \"some data\" \\\n--location mymachine\n</code></pre> <p>The option <code>--data_prep 2</code> specifies which data preparation container will be used in later steps to prepare your dataset. We are using the same container submitted by the training experiment owner.</p>"},{"location":"training_tutorial/#process-your-data-using-the-data-preparation-container","title":"Process your data using the data preparation container","text":"<p>Now run preparation to transform your data as required by the experiment admin. Your dataset ID will be <code>1</code>. You can check by running <code>medperf dataset ls --mine</code>.</p> <pre><code>medperf dataset prepare --data_uid 1 </code></pre>"},{"location":"training_tutorial/#mark-your-dataset-as-ready","title":"Mark your dataset as ready","text":"<p>Now mark your dataset as ready for training:</p> <pre><code>medperf dataset set_operational --data_uid 1\n</code></pre> <p>This will also submit some statistics calculated on your data that could be useful for results analysis by the experiment admin.</p>"},{"location":"training_tutorial/#6-data-owner-requesting-participation","title":"6. Data Owner: Requesting participation","text":"<p>Now request participation in the training experiment:</p> <pre><code>medperf dataset associate  --data_uid 1 --training_exp_uid 1\n</code></pre>"},{"location":"training_tutorial/#7-data-owner-start-the-training-node","title":"7. Data Owner: Start the training node","text":"<p>Finally, start the training node:</p> <pre><code>medperf dataset train --data_uid 1 --training_exp_id 1\n</code></pre> <p>The process will keep waiting until the experiment admin submits the training job and signals your node to start training. Please keep this terminal open. You should move now to another terminal.</p>"},{"location":"training_tutorial/#8-data-owner-setting-up-the-second-data-owner","title":"8. Data Owner: Setting up the second data owner","text":"<p>For the second data owner, you will do the same steps as the first data owner. To avoid repetition, we provide a script that you can run to quickly run what needs to be run by the second data owner:</p> <pre><code>bash medperf_tutorial/collab_shortcut.sh\n</code></pre> <p>You should move now to another terminal</p>"},{"location":"training_tutorial/#9-experiment-admin-start-the-training","title":"9. Experiment Admin: Start the training","text":"<p>Now you have the aggregator and the two data owner nodes up and running. You can now as an experiment admin signal starting the training. Login in back as the admin to continue:</p> <pre><code>medperf auth logout\nmedperf auth login -e testmo@example.com\n</code></pre>"},{"location":"training_tutorial/#submit-the-training-job","title":"Submit the training job","text":"<pre><code>medperf training submit_job --training_exp_id 1\n</code></pre> <p>After running this, you can go and check that the terminals of the data owners and the aggregator are now showing that they are doing federated training.</p>"},{"location":"training_tutorial/#10-experiment-admin-remote-status-check","title":"10. Experiment Admin: Remote status check","text":"<p>In a real experiment, you probably won't have access to data owners terminals to see if everything is going fine. You can run the following command to check the status from your machine:</p> <pre><code>medperf training get_experiment_status --training_exp_id 1\n</code></pre>"},{"location":"training_tutorial/#11-experiment-admin-stop-the-training","title":"11. Experiment Admin: Stop the training","text":"<p>After training is done, or if you want to just interrupt the training and stop it, you can run the following command:</p> <pre><code>medperf training close_event --training_exp_id 1\n</code></pre> <p>This concludes our tutorial!</p>"},{"location":"tutorial/","title":"Federated Evaluation Tutorial","text":"<p>Please follow the tutorial on this page</p>"},{"location":"archive/2022_index/","title":":warning: Copied from previous FL-Tutorial page :warning:","text":""},{"location":"archive/2022_index/#federated-learning-for-healthcare","title":"Federated Learning for Healthcare","text":"<p>Welcome to the Federated Learning tutorial that will be run in conjunction with the MICCAI conference!</p> <p>Federated Learning (FL) is increasingly important in privacy sensitive domains, such as healthcare, where sharing of private/patient data is a barrier to building models that generalize well in the real world and minimize bias.  </p> <p>In 2021, we led the largest real world federation, with a network of 59 healthcare institutions around the world. Furthermore, leveraging the collaborators of this real-world FL initiative, we led the first ever FL challenge, focusing on the tumor segmentation task, called The FeTS 2021 challenge. Taking into consideration the value and the interest of the community in this new paradigm for data private multi-institutional collaborations and building upon our experience, we organize this tutorial on FL for healthcare.</p> <p>The tutorial will teach you how to use state-of-the-art open-source Python library for Federated Learning OpenFL.</p>"},{"location":"archive/2022_index/#when-and-where","title":"When and Where?","text":"<p>September 22nd from 11:50 to 15:20 (SGT time)</p> <p>Sign up as a virtual tutorial attendee until the day of the event at the MICCAI Registration Page</p>"},{"location":"archive/2022_index/#organizing-committee","title":"Organizing Committee","text":"<ul> <li>Ujjwal Baid, University of Pennsylvania.</li> <li>Spyridon Bakas, University of Pennsylvania.</li> <li>Patrick Foley, Intel Corporation.</li> <li>Sarthak Pati, University of Pennsylvania.</li> <li>Prashant Shah, Intel Corporation.</li> <li>Mansi Sharma, Intel Corporation.</li> <li>Micah Sheller, Intel Corporation.</li> <li>Karan Shah, Intel Corporation.</li> </ul>"},{"location":"archive/2022_index/#tutorial-description","title":"Tutorial Description","text":"<p>The aim of this tutorial is to facilitate education on how to perform Federated Learning on both simulated and real-world studies. Tutorial structure focuses on specific clearly indicated parts for beginners and for more advanced attendees. Data scientists of different medical imaging communities (e.g., radiology, pathology) are considered during this tutorial on the opportunities and challenges in developing and using FL for training Al models across institutions using privacy preserving techniques. We plan on covering a spectrum of techniques, from software-based approaches that can be considered a method or a metric (e.g., differential privacy), to hardware-based trusted execution computing environments (TEEs).</p> <p>The motivation for the tutorial is driven by the need to train and validate deep learning models across data silos, to create models that gain knowledge from diverse patient populations and hence generalize well, mitigate bias, and pave the way towards addressing health disparities.</p>"},{"location":"archive/2022_index/#preliminary-program","title":"Preliminary program","text":""},{"location":"archive/2022_index/#part-i-45-minutes-environment-setup","title":"Part I (45 minutes, environment setup)","text":"<ul> <li>Get access to the Google Colab enviroment</li> </ul>"},{"location":"archive/2022_index/#part-ii-45-minutes-lecture-based","title":"Part II (45 minutes, lecture based)","text":"<ul> <li>Introduction to FL</li> <li>Considerations for FL, based on what we learned from: </li> <li>The largest known real-world global federation FeTS, and</li> <li>The first ever proposal challenge on federated learning MICCAI 2021 FeTS challenge.</li> </ul>"},{"location":"archive/2022_index/#part-iii-2h-hands-on","title":"Part III (~2h, Hands on)","text":"<ul> <li>Hands-on and interactive tutorial on simulating federations and training various segmentation and classification models, while taking into account numerous considerations (including but not limited to a) data size across collaborators, b) network delays in sharing model weights).</li> </ul> <p>Data Scientists and Computational Scientists attending this tutorial will be able to adapt their existing centralized algorithms to a federated architecture or build new models. Non-data scientists (e.g., more clinically-oriented attendees of the CLINICAI sessions) will learn about both technical and non-technical considerations setting up federations for training medical Al models. Importantly, attendees will also understand the privacy and security attack vectors and mitigations when using FL.</p>"},{"location":"archive/2022_index/#speakers","title":"Speakers","text":"<ul> <li> <p>Spyridon Bakas, Ph.D., is an Assistant Professor at the University of Pennsylvania, focusing on computational algorithms for oncological imaging, towards improving the clinical practice.</p> </li> <li> <p>Patrick Foley is a Senior Deep Learning (DL) Software Engineer at Intel and Lead Architect of OpenFL, an open-source library for FL.</p> </li> <li> <p>Mansi Sharma is a Deep Learning (DL) Software Engineer at Intel  and a developer of OpenFL, an open-source library for FL.</p> </li> <li> <p>Karan Shah is an Applied Machine Learning Engineer at Intel and a developer of OpenFL. His interests span Deep Learning, Optimization Theory, Statistics and Cosmology.</p> </li> <li> <p>Sarthak Pati, M.Sc., is a Sr. Application Developer at UPenn. He focuses on ML, distributed, and privacy-protected algorithms for healthcare, and currently leads the R&amp;D of the FeTS platform.</p> </li> </ul>"},{"location":"archive/2022_index/#format","title":"Format","text":"<p>Half day (afternoon), Hands-on</p>"},{"location":"archive/2022_index/#proceedings","title":"Proceedings","text":"<p>In favor of open science, transparency, and further communicating the information presented during the tutorial beyond its actual lifecycle during MICCAI 2021, we intend to produce tutorial notes and be part of the MICCAI satellite event proceedings.</p>"},{"location":"archive/2023_rsna_tutorial/","title":"Tutorial","text":""},{"location":"archive/2023_rsna_tutorial/#requirements","title":"Requirements","text":"<p>To run this tutorial, you need a machine with the following requirements:</p> <ul> <li>Internet access.</li> <li>Web browser (to connect to codespaces).</li> </ul>"},{"location":"archive/2023_rsna_tutorial/#training-setup-with-medperf-model-owner","title":"Training Setup with MedPerf (Model Owner)","text":"<pre><code>medperf auth login -e modelowner@example.com\n</code></pre>"},{"location":"archive/2023_rsna_tutorial/#define-the-data-preparation-mlcube","title":"Define the data preparation MLCube","text":"<ul> <li>Prepare the data preparation pipeline logic that will transform the raw clinical data into AI-ready data. This will be an MLCube</li> </ul>"},{"location":"archive/2023_rsna_tutorial/#register-the-mlcube","title":"Register the MLCube","text":"<pre><code>medperf mlcube submit -n prep \\\n-m https://storage.googleapis.com/medperf-storage/rsna2023/mlcube.yaml\n</code></pre>"},{"location":"archive/2023_rsna_tutorial/#define-the-training-mlcube","title":"Define the training MLCube","text":"<ul> <li>Prepare the training logic using OpenFL and GaNDLF</li> </ul>"},{"location":"archive/2023_rsna_tutorial/#register-the-training-mlcube","title":"Register the Training MLCube","text":"<pre><code>medperf mlcube submit -n testfl \\\n-m https://storage.googleapis.com/medperf-storage/rsna2023/mlcube_rsna.yaml \\\n-p https://storage.googleapis.com/medperf-storage/rsna2023/plan_final.yaml \\\n-a https://storage.googleapis.com/medperf-storage/rsna2023/init_weights_rsna2023.tar.gz\n</code></pre>"},{"location":"archive/2023_rsna_tutorial/#register-the-training-experiment","title":"Register the Training Experiment","text":"<pre><code>medperf training submit -n trainexp -d trainexp -p 1 -m 2\n</code></pre> <p>The server admin should approve the experiment. Run:</p> <pre><code>bash admin_training_approval.sh\n</code></pre>"},{"location":"archive/2023_rsna_tutorial/#aggregator-setup-with-medperf-aggregator-owner","title":"Aggregator Setup with MedPerf (Aggregator Owner)","text":"<pre><code>medperf auth login -e aggowner@example.com\n</code></pre>"},{"location":"archive/2023_rsna_tutorial/#register-aggregator","title":"register aggregator","text":"<pre><code>medperf aggregator submit -n aggreg -a $(hostname --fqdn) -p 50273\n</code></pre>"},{"location":"archive/2023_rsna_tutorial/#associate-the-aggregator-with-the-experiment","title":"Associate the aggregator with the experiment","text":"<pre><code>medperf aggregator associate -a 1 -t 1\n</code></pre>"},{"location":"archive/2023_rsna_tutorial/#data-preparation-training-data-owner","title":"Data preparation (Training Data Owner)","text":"<pre><code>medperf auth login -e traincol1@example.com\n</code></pre>"},{"location":"archive/2023_rsna_tutorial/#process-your-data-using-the-data-prep-mlcube","title":"Process your data using the data prep mlcube","text":"<pre><code>medperf dataset create -p 1 -d datasets/col1 -l datasets/col1 --name col1 --description col1data --location col1location\n</code></pre>"},{"location":"archive/2023_rsna_tutorial/#register-your-dataset","title":"Register your dataset","text":"<p>find Hash:</p> <pre><code>medperf dataset ls\n</code></pre> <pre><code>medperf dataset submit -d &lt;hash_found&gt;\n</code></pre>"},{"location":"archive/2023_rsna_tutorial/#request-participation-in-the-training-experiment","title":"Request participation in the training experiment","text":"<pre><code>medperf training associate_dataset -t 1 -d 1\n</code></pre>"},{"location":"archive/2023_rsna_tutorial/#redo-the-same-with-collaborator-2","title":"Redo the same with collaborator 2","text":"<pre><code>bash shortcut.sh\n</code></pre>"},{"location":"archive/2023_rsna_tutorial/#accepting-training-participation-model-owner","title":"Accepting Training Participation (Model Owner)","text":"<pre><code>medperf auth login -e modelowner@example.com\n</code></pre>"},{"location":"archive/2023_rsna_tutorial/#accept-participation-requests","title":"Accept participation requests","text":"<pre><code>medperf training approve_association -t 1 -a 1\nmedperf training approve_association -t 1 -d 1\nmedperf training approve_association -t 1 -d 2\n</code></pre>"},{"location":"archive/2023_rsna_tutorial/#lock-the-experiment","title":"Lock the experiment","text":"<pre><code>medperf training lock -t 1\n</code></pre>"},{"location":"archive/2023_rsna_tutorial/#run-the-aggregator-aggregator-owner","title":"Run the Aggregator (Aggregator Owner)","text":"<pre><code>medperf auth login -e aggowner@example.com\n</code></pre> <pre><code>medperf aggregator start -a 1 -t 1\n</code></pre> <p>(Now move to another terminal)</p>"},{"location":"archive/2023_rsna_tutorial/#run-training-training-data-owner","title":"Run Training (Training Data Owner)","text":"<p>First collaborator:</p> <pre><code>medperf auth login -e traincol1@example.com\n</code></pre> <pre><code>medperf training run -d 1 -t 1\n</code></pre> <p>(Now move to another terminal)</p> <p>Second collaborator:</p> <pre><code>medperf auth login -e traincol2@example.com\n</code></pre> <pre><code>medperf training run -d 2 -t 1\n</code></pre>"},{"location":"archive/2024_aaai/","title":"Schedule for AAAI 2024","text":"Time Session Speaker Comments 0900 - 0905 Environment Setup Hasan/Sarthak Codespaces should make this possible. 0905 - 0935 Introduction to Federated Learning for Healthcare Spyros/Ezequiel 25 min talk + Q&amp;A 0935 - 1000 Federated Learning in Heterogenous Settings Xiaxiao 20 min talk + Q&amp;A 1000 - 1030 Introduction to Model Design using GaNDLF Sarthak 25 min talk + Q&amp;A 1030 - 1100 Break and Questions related to Environment Setup N.A. Light refreshments available  near session rooms 1100 - 1120 Introduction to Model Optimization Ezequiel 20 min talk + Q&amp;A 1120 - 1140 Hands on with Model Optimization Ezequiel Lecture + demo using slides &amp; Codespaces 1140 - 1230 Federated Learning using OpenFL Federated Evaluation using MedPerf Hasan Demo using slides &amp; Codespaces 1230 - 1400 Lunch break N.A. On your own 1400 - onwards Open discussion with organizers N.A. N.A."},{"location":"archive/2024_isbi/","title":"Schedule for ISBI 2024","text":"Time Session Speaker Comments 0900 - 0905 Environment Setup Hasan/Sarthak Codespaces should make this possible. 0905 - 1000 Introduction to Federated Learning for Healthcare Spyros/Walter 45 min talk + Q&amp;A 1000 - 1030 Introduction to Model Design using GaNDLF Sarthak 25 min talk + Q&amp;A 1030 - 1100 Break and Questions related to Environment Setup N.A. Light refreshments available  near session rooms 1100 - 1120 Introduction to Model Optimization Siddhesh 20 min talk + Q&amp;A 1120 - 1140 Hands on with Model Optimization Siddhesh Lecture + demo using slides &amp; Codespaces 1140 - 1230 Federated Learning using OpenFL Federated Evaluation using MedPerf Hasan Demo using slides &amp; Codespaces 1230 - 1400 Lunch break N.A. On your own 1400 - onwards Open discussion with organizers N.A. N.A."},{"location":"archive/2024_miccai/","title":"Schedule for MICCAI 2024","text":"Time Title Speaker Comments 0800 - 0845 Introduction to Federated Learning for Healthcare Spyridon Bakas 40' talk + Q&amp;A 0845 - 0930 Challenges and Practical Use Cases for Federated Learning in Medical Imaging Qi Dou 40' talk + Q&amp;A 0930 - 1000 Introduction to Model Design using GaNDLF Sarthak Pati 25' talk + Q&amp;A 1000 - 1030 Break 1030 - 1100 Model optimization using OpenVINO Siddhesh Thakur 25' talk + Q&amp;A 1100 - 1130 Model deployment using Hugging Face Cyril Zakka 25' talk + Q&amp;A 1130 - 1200 Federated Learning using OpenFL Patrick Foley 25' talk + Q&amp;A 1200 - 1230 Federated Evaluation using MedPerf and how to conduct hands-on session Hasan Kassem 25' talk + Q&amp;A <p>All presentation material can be found here.</p> <p>The self-guided hands-on session can be found here.</p>"},{"location":"archive/2025_miccai/","title":"2025 miccai","text":"<p>To be filled</p>"}]}